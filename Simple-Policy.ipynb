{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faffcaf2",
   "metadata": {},
   "source": [
    "# The Multi-armed bandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b29261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import tf_slim as slim\n",
    "import numpy as np\n",
    "tf.compat.v1.disable_eager_execution()  # eager execution 관련 에러 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a30ebd",
   "metadata": {},
   "source": [
    "### The Bandit\n",
    "- 손잡이가 4개인 밴딧\n",
    "- pullBandit 함수 : 랜덤 표준정규분포 값 생성하고 인수로 받은 수보다 크면 1, 아니면 -1 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d246716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 밴딧의 손잡이 목록 작성\n",
    "# 현재 손잡이 4 (인덱스 #3)이 가장 자주 양의 보상 제공하도록 설정되어있음\n",
    "bandit_arms = [0.2, 0, -0.2, -2]\n",
    "num_arms = len(bandit_arms)\n",
    "def pullBandit(bandit):\n",
    "    # 랜덤한 표준정규분포값 생성\n",
    "    result = np.random.randn(1)\n",
    "    if result > bandit:\n",
    "        # 양의 보상 반환\n",
    "        return 1\n",
    "    else:\n",
    "        # 음의 보상 반환\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b45c142",
   "metadata": {},
   "source": [
    "### The Agent\n",
    "- 간단한 신경망 구현\n",
    "- 각 손잡이에 대한 일련의 값들로 구성\n",
    "- 각 값은 해당 뱃딧을 선택할 때 반환되는 보상의 추정값\n",
    "- 정책 경사 방법을 이용해 큰 보상을 받도록 에이전트 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b3fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# 네트워크의 피드포워드 부분 구현\n",
    "weights = tf.Variable(tf.ones([num_arms]))  # num_arms = 4\n",
    "output = tf.nn.softmax(weights)  # 활성함수가 softmax인 신경망\n",
    "\n",
    "# 학습 과정 구현\n",
    "# 보상과 선택된 액션을 네트워크에 피드해줌으로써 비용(loss) 계산하고 \n",
    "# 비용을 이용해 네트워크 업데이트\n",
    "\n",
    "reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n",
    "action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n",
    "\n",
    "responsible_output = tf.slice(output, action_holder, [1])   #tf.slice(input_, begin,size)\n",
    "loss = -(tf.log(responsible_output)*reward_holder)  # Loss = -log(pi)*A\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f7680",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "- 이 환경에서 에이전트는 액션을 취함으로써 학습하고 보상을 받게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec5a06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reward for the 4arms of the bandit: [1. 0. 0. 0.]\n",
      "Running reward for the 4arms of the bandit: [-7. -1. -3.  8.]\n",
      "Running reward for the 4arms of the bandit: [-14.  -1.   4.  20.]\n",
      "Running reward for the 4arms of the bandit: [-17.   3.  14.  35.]\n",
      "Running reward for the 4arms of the bandit: [-14.   7.  17.  49.]\n",
      "Running reward for the 4arms of the bandit: [-13.   9.  28.  57.]\n",
      "Running reward for the 4arms of the bandit: [-17.   8.  33.  67.]\n",
      "Running reward for the 4arms of the bandit: [-22.   8.  35.  78.]\n",
      "Running reward for the 4arms of the bandit: [-26.   6.  41.  86.]\n",
      "Running reward for the 4arms of the bandit: [-27.   1.  42.  95.]\n",
      "Running reward for the 4arms of the bandit: [-30.   8.  45. 104.]\n",
      "Running reward for the 4arms of the bandit: [-33.   6.  49. 123.]\n",
      "Running reward for the 4arms of the bandit: [-35.   9.  50. 135.]\n",
      "Running reward for the 4arms of the bandit: [-35.   9.  51. 148.]\n",
      "Running reward for the 4arms of the bandit: [-44.  10.  57. 156.]\n",
      "Running reward for the 4arms of the bandit: [-43.   9.  58. 173.]\n",
      "Running reward for the 4arms of the bandit: [-46.  12.  65. 188.]\n",
      "Running reward for the 4arms of the bandit: [-50.  12.  61. 204.]\n",
      "Running reward for the 4arms of the bandit: [-54.  16.  59. 216.]\n",
      "Running reward for the 4arms of the bandit: [-56.  16.  55. 232.]\n",
      "\n",
      "The agent thinks arm 4 is the most promising....\n",
      "...and it was right!\n"
     ]
    }
   ],
   "source": [
    "total_episodes = 1000  # 에이전트를 학습시킬 총 에피소드 수\n",
    "total_reward = np.zeros(num_arms)  # 밴딧 손잡이에 대한 점수판을 0으로 설정\n",
    "\n",
    "# 그래프 실행 전 초기화를 해야 그 값이 변수에 지정됨\n",
    "init = tf.global_variables_initializer() \n",
    "\n",
    "# 텐서플로 그래프 론칭\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    i = 0\n",
    "    while i < total_episodes:  # 에피소드 수만큼 반복 (1000회)\n",
    "        # 볼츠만(Boltzmann) 분포에 따라 액션 선택\n",
    "        actions = sess.run(output)\n",
    "        a = np.random.choice(actions, p=actions)\n",
    "        action = np.argmax(actions==a)\n",
    "        \n",
    "        reward = pullBandit(bandit_arms[action])  # 손잡이 중 하나를 선택하여 보상을 받음\n",
    "        \n",
    "        # 네트워크 업데이트\n",
    "        _, resp, ww = sess.run([update, responsible_output, weights], feed_dict={reward_holder:[reward], action_holder:[action]})\n",
    "        \n",
    "        # 보상의 총계 업데이트\n",
    "        total_reward[action] += reward\n",
    "        if i%50 == 0:\n",
    "            print(\"Running reward for the \" + str(num_arms) + \"arms of the bandit: \" + str(total_reward))\n",
    "        i += 1\n",
    "        \n",
    "    print(\"\\nThe agent thinks arm \" + str(np.argmax(ww)+1) + \" is the most promising....\")\n",
    "    if np.argmax(ww) == np.argmax(-np.array(bandit_arms)):\n",
    "        print(\"...and it was right!\")\n",
    "    else:\n",
    "        print(\"... and it was wrong!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflowRL",
   "language": "python",
   "name": "tensorflowrl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
